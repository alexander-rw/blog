// Bring EMBEDDED_MDX into scope from the file generated by build.rs.
// Each entry is (url_key, mdx_source) with the source embedded at compile time.
include!(concat!(env!("OUT_DIR"), "/generated_pages.rs"));

use std::collections::HashMap;

use anyhow::{Context, Result};

use crate::{mdx_options, page, parse_mdx, post, template};

/// All pre-rendered pages, built once at server startup from the MDX sources
/// embedded in the binary by `build.rs`.
///
/// Handlers look up pages by URL key (the path segment after `/`) instead of
/// reading or parsing files at request time.
pub struct PageStore {
    /// URL key → pre-rendered full-page HTML.
    pages: HashMap<String, String>,
    /// Pre-rendered blog post listing page.
    blog_listing: String,
}

/// Intermediate data produced during the first parse pass, before rendering.
///
/// Owned so it can outlive the parsing loop and be used in the render pass.
struct PageData {
    key: String,
    meta: page::PageMeta,
    html_content: String,
    listing: Option<post::PostListing>,
}

impl PageStore {
    /// Parse and render every embedded MDX source, returning a fully built store.
    ///
    /// Uses a two-pass strategy:
    /// 1. Parse all MDX files, extract metadata and post listings (with body text).
    /// 2. Serialise the search index, then render every page with the index inline.
    ///
    /// This ordering is required because each rendered page embeds the *complete*
    /// search index covering all posts — so the index must be finalised before any
    /// page is rendered.
    ///
    /// Called once at server startup. Any validation or render failure returns
    /// `Err`, preventing the server from starting with broken content.
    ///
    /// # Errors
    ///
    /// Returns an error if any MDX file fails frontmatter validation, YAML
    /// deserialisation, or HTML rendering.
    pub fn build() -> Result<Self> {
        // `markdown::Options` is `!Send` (holds `Box<dyn Fn(...)>`), so it must
        // be constructed and consumed on the same thread. Here that is guaranteed
        // because `build()` is a plain synchronous function.
        let opts = mdx_options::default_mdx_compile_options();

        // --- Pass 1: parse every MDX source, collecting metadata and listings. ---
        // Rendering is deferred until the full search index is available.
        let mut parsed: Vec<PageData> = Vec::with_capacity(EMBEDDED_MDX.len());

        for &(key, source) in EMBEDDED_MDX {
            let ast = parse_mdx(source)
                .map_err(|e| anyhow::anyhow!("{e}"))
                .with_context(|| format!("parsing MDX for \"{key}\""))?;

            let meta = page::extract_meta(&ast, source)
                .map_err(|e| anyhow::anyhow!("{e}"))
                .with_context(|| format!("extracting metadata for \"{key}\""))?;

            let html_content = markdown::to_html_with_options(source, &opts)
                .map_err(|e| anyhow::anyhow!("{e}"))
                .with_context(|| format!("rendering HTML for \"{key}\""))?;

            // Any page filed under blog/ is a candidate for the post listing.
            // `extract_listing` returns None for drafts so they are silently skipped.
            let listing = if let Some(slug) = key.strip_prefix("blog/") {
                post::extract_listing(slug, &ast)
                    .map_err(|e| anyhow::anyhow!("{e}"))
                    .with_context(|| format!("extracting post listing for \"{key}\""))?
            } else {
                None
            };

            parsed.push(PageData {
                key: key.to_owned(),
                meta,
                html_content,
                listing,
            });
        }

        // --- Build the search index from all collected post listings. ---
        // Clone listings out of `parsed` so we can sort them independently.
        let mut post_listings: Vec<post::PostListing> =
            parsed.iter().filter_map(|p| p.listing.clone()).collect();

        // Descending date sort; posts without a date sink to the bottom.
        post_listings.sort_by(|a, b| match (&b.date, &a.date) {
            (Some(bd), Some(ad)) => bd.cmp(ad),
            (Some(_), None) => std::cmp::Ordering::Less,
            (None, Some(_)) => std::cmp::Ordering::Greater,
            (None, None) => std::cmp::Ordering::Equal,
        });

        // Serialise to JSON; escape `</` as `<\/` (valid JSON, prevents accidental
        // `</script>` tag closure when the JSON is inlined in an HTML <script> block).
        let search_json = serde_json::to_string(&post_listings)
            .context("serialising search index")?
            .replace("</", "<\\/");

        // --- Pass 2: render every page with the complete search index inline. ---
        let mut pages = HashMap::with_capacity(parsed.len());
        for p in &parsed {
            pages.insert(
                p.key.clone(),
                template::render_page(&p.meta, &p.html_content, &search_json).0,
            );
        }

        let blog_listing = template::render_post_list(&post_listings, &search_json).0;

        Ok(PageStore {
            pages,
            blog_listing,
        })
    }

    /// Look up a pre-rendered page by URL key.
    ///
    /// Returns `None` if no page was compiled for the given key.
    pub fn page(&self, key: &str) -> Option<&str> {
        self.pages.get(key).map(String::as_str)
    }

    /// Return the pre-rendered blog post listing page.
    pub fn blog_listing(&self) -> &str {
        &self.blog_listing
    }
}
